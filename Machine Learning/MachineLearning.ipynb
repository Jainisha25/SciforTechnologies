{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Regression:"
      ],
      "metadata": {
        "id": "c0Do4AdRhgR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is a linear approach to modelling the relationship between a dependent variable and one or more independent variables."
      ],
      "metadata": {
        "id": "_pGDNE3ihi_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "soTC4T3rhJCA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros((n_features,1))\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            y_predicted = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Generate sample data\n",
        "    np.random.seed(0)\n",
        "    X = 2 * np.random.rand(100, 1)\n",
        "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "    # Instantiate and fit the model\n",
        "    model = LinearRegression(learning_rate=0.01, n_iterations=1000)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    X_test = np.array([[0], [2]])\n",
        "    predictions = model.predict(X_test)\n",
        "    print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0veCf0LvjA7k",
        "outputId": "2e2aafff-3bdc-4f9d-c71b-2387e893aa8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [[ 4.04468715]\n",
            " [10.29656019]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiple Linear Regression:"
      ],
      "metadata": {
        "id": "DAyUIVfFt3qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regression (MLR) is an extension of simple linear regression that involves predicting a continuous target variable based on two or more predictor variables. In MLR, the relationship between the target variable and multiple predictors is modeled as a linear combination of the predictors."
      ],
      "metadata": {
        "id": "EkhC0RRvt_fF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree:"
      ],
      "metadata": {
        "id": "nZPjsVWhumG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree is a supervised learning algorithm used for both classification and regression tasks. It works by recursively partitioning the feature space into smaller regions, making decisions based on the values of the features at each node. The goal is to create a tree that predicts the target variable by splitting the data into subsets that are as homogeneous as possible."
      ],
      "metadata": {
        "id": "NGzdH1QoutZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature  # Index of feature to split on\n",
        "        self.threshold = threshold  # Threshold value for the feature\n",
        "        self.left = left  # Left subtree\n",
        "        self.right = right  # Right subtree\n",
        "        self.value = value  # Class label for leaf node\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._grow_tree(X, y, depth=0)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping criteria\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or n_classes == 1:\n",
        "            return Node(value=np.bincount(y).argmax())\n",
        "\n",
        "        # Find the best split\n",
        "        best_gini = float('inf')\n",
        "        best_feature, best_threshold = None, None\n",
        "        for feature in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature] < threshold\n",
        "                gini = self._gini_impurity(y[left_indices], y[~left_indices])\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        # Create split\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        left = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right = self._grow_tree(X[~left_indices], y[~left_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left, right=right)\n",
        "\n",
        "    def _gini_impurity(self, left_y, right_y):\n",
        "        n_left, n_right = len(left_y), len(right_y)\n",
        "        n_total = n_left + n_right\n",
        "        p_left = np.bincount(left_y, minlength=n_total) / n_left\n",
        "        p_right = np.bincount(right_y, minlength=n_total) / n_right\n",
        "        gini_left = 1.0 - np.sum(p_left ** 2)\n",
        "        gini_right = 1.0 - np.sum(p_right ** 2)\n",
        "        return (n_left * gini_left + n_right * gini_right) / n_total\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_single(x, self.root) for x in X])\n",
        "\n",
        "    def _predict_single(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature] < node.threshold:\n",
        "            return self._predict_single(x, node.left)\n",
        "        else:\n",
        "            return self._predict_single(x, node.right)"
      ],
      "metadata": {
        "id": "C3mL4dEKuHq4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Generate sample data\n",
        "    np.random.seed(0)\n",
        "    X = np.random.rand(100, 2)\n",
        "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
        "\n",
        "    # Instantiate and fit the model\n",
        "    model = DecisionTreeClassifier(max_depth=3)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    X_test = np.array([[0.5, 0.5], [1.5, 1.5]])\n",
        "    predictions = model.predict(X_test)\n",
        "    print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is-ilpTOu1j6",
        "outputId": "857ed213-691f-4fee-cff4-b31f16db2539"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-bea88786e58b>:49: RuntimeWarning: invalid value encountered in divide\n",
            "  p_left = np.bincount(left_y, minlength=n_total) / n_left\n",
            "<ipython-input-15-bea88786e58b>:49: RuntimeWarning: divide by zero encountered in divide\n",
            "  p_left = np.bincount(left_y, minlength=n_total) / n_left\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naive Bayes:"
      ],
      "metadata": {
        "id": "HDBKv2__u4kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a probabilistic classifier based on Bayes' theorem with the \"naive\" assumption of independence between features. It's commonly used for classification tasks, especially in text classification and spam filtering."
      ],
      "metadata": {
        "id": "io1UW-h6u-3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        self.mean = np.zeros((len(self.classes), n_features))\n",
        "        self.var = np.zeros((len(self.classes), n_features))\n",
        "        self.priors = np.zeros(len(self.classes))\n",
        "\n",
        "        for idx, c in enumerate(self.classes):\n",
        "            X_c = X[y == c]\n",
        "            self.mean[idx, :] = X_c.mean(axis=0)\n",
        "            self.var[idx, :] = X_c.var(axis=0)\n",
        "            self.priors[idx] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "    def _calculate_likelihood(self, X, class_idx):\n",
        "        mean = self.mean[class_idx]\n",
        "        var = self.var[class_idx]\n",
        "        numerator = np.exp(- (X - mean) ** 2 / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def _calculate_posterior(self, X):\n",
        "        posteriors = []\n",
        "        for idx, _ in enumerate(self.classes):\n",
        "            prior = np.log(self.priors[idx])\n",
        "            likelihood = np.sum(np.log(self._calculate_likelihood(X, idx)), axis=1)\n",
        "            posterior = prior + likelihood\n",
        "            posteriors.append(posterior)\n",
        "        return posteriors\n",
        "\n",
        "    def predict(self, X):\n",
        "        posteriors = self._calculate_posterior(X)\n",
        "        return self.classes[np.argmax(posteriors, axis=0)]"
      ],
      "metadata": {
        "id": "bD5kcKpHu2Rf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Generate sample data\n",
        "    np.random.seed(0)\n",
        "    X = np.random.randn(100, 2)\n",
        "    y = np.random.randint(0, 2, 100)\n",
        "\n",
        "    # Instantiate and fit the model\n",
        "    model = GaussianNaiveBayes()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    X_test = np.array([[0, 0], [1, 1]])\n",
        "    predictions = model.predict(X_test)\n",
        "    print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUbM25TNvGB2",
        "outputId": "549ca6d3-55ec-4b1e-cb75-ddd9ede53c90"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM:"
      ],
      "metadata": {
        "id": "PWBpaLuavOw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. It works by finding the hyperplane that best separates the classes in the feature space. SVM aims to maximize the margin between the hyperplane and the nearest data points (support vectors) of each class while minimizing the classification error."
      ],
      "metadata": {
        "id": "xArtWjNzvQPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate the SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnPpWqmuvXqz",
        "outputId": "2c6e67c3-06aa-41ca-9eda-e649da3c96df"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fG4H8h_1vxpk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}